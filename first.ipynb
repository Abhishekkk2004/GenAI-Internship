{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING LOCAL LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model='llama3.2:3b', base_url='http://localhost:11434')\n",
    "llm1 = ChatOllama(model='llama3.2:3b', base_url='http://localhost:11434')\n",
    "llm2 = ChatOllama(model='deepseek-r1:1.5b', base_url='http://localhost:11434')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='nomic-embed-text', base_url='http://localhost:11434')\n",
    "\n",
    "# db_name = r\"D:\\NLP\\LLM\\Langchain and Ollama\\09. Vector Stores and Retrievals\\health_supplements\"\n",
    "# vector_store = FAISS.load_local(db_name, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain Routing between two LLMS differentiating between Reasoning and Questioning Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Given the input below, classify it as either `Reason` if it involves reasoning, explanations, justifications, opinions, or causal statements,\n",
    "            or `Question` if it is a direct inquiry seeking factual or specific information.\n",
    "            \n",
    "            If the input contains a mix of reasoning and a question, classify it as `Reason`.\n",
    "\n",
    "            Input: {user_input}\n",
    "            Classification:\"\"\"\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_prompt = \"\"\"\n",
    "                You are an expert in analyzing and responding to reasoning-based inputs.\n",
    "                Your task is to acknowledge the reasoning, provide a concise and relevant response, \n",
    "                and encourage further discussion if needed.\n",
    "                             \n",
    "                Input: {user_input}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "reason_template = ChatPromptTemplate.from_template(reason_prompt)\n",
    "reason_chain =  reason_template | llm2 | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt = \"\"\"\n",
    "                You are an expert in answering direct questions concisely and accurately.\n",
    "                Your task is to provide a clear, well-structured, and informative response to the user's question.\n",
    "                \n",
    "                Question: {user_input}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "question_template = ChatPromptTemplate.from_template(question_prompt)\n",
    "question_chain = question_template | llm1 | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rout(info):\n",
    "    question_type = info['question_type'].strip().lower()\n",
    "    if question_type == \"reason\":\n",
    "        print(\"Routing to: Reason Chain (Using Deepseek R1)\")  # Debug print\n",
    "        return reason_chain\n",
    "    else:\n",
    "        print(\"Routing to: Question Chain (Using Llama3.2)\")  # Debug print\n",
    "        return question_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_chain = {\"question_type\": chain, 'user_input': lambda x: x['user_input']} | RunnableLambda(rout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  question_type: ChatPromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='Given the input below, classify it as either `Reason` if it involves reasoning, explanations, or justifications, \\n            or `Question` if it is a direct question seeking information.\\n            \\n            Input: {user_input}\\n            Classification:'), additional_kwargs={})])\n",
       "                 | ChatOllama(model='llama3.2:3b', base_url='http://localhost:11434')\n",
       "                 | StrOutputParser(),\n",
       "  user_input: RunnableLambda(lambda x: x['user_input'])\n",
       "}\n",
       "| RunnableLambda(rout)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing to: Question Chain (Using llm1)\n",
      "Yes, I agree. Cricket's popularity can be attributed to its unique combination of individual skill, tactical complexity, and reliance on effective team coordination. The intricacies of the game, including the various playing formats (e.g., Test matches, One-Day Internationals, Twenty20), make it appealing to players and spectators alike due to the need for a balance between physical prowess and mental acuity.\n"
     ]
    }
   ],
   "source": [
    "# user_input= \"Cricket is a popular sport because it requires teamwork, strategy, and skill. Do you agree?\"  \n",
    "\n",
    "# output = full_chain.invoke({'user_input': user_input})\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Vector Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Chunks of the wikipedia_context data and using RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load Wikipedia text from file\n",
    "with open(\"/Users/abhishek/Desktop/ML/GenAI/wikipedia_content.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    wikipedia_text = file.read()\n",
    "\n",
    "# Wrap text in a Document object\n",
    "docs = [Document(page_content=wikipedia_text)]\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200)\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embeddings.embed_query(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 768)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)\n",
    "index = faiss.IndexFlatL2(len(vector))\n",
    "index.ntotal, index.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INDEXING OF THE SCRAPPED DATA INTO CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 768)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index.ntotal, vector_store.index.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vector_store.add_documents(documents=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 71)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids), vector_store.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retreival\n",
    "question = \"What is a transformer?\"\n",
    "docs = vector_store.search(query=question, k=5, search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fb988468-ee5f-4a92-93f4-412e4d0d0c8a', metadata={}, page_content='In 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models,[38] leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018.[39] This was followed in 2019 by GPT-2 which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.[40]'),\n",
       " Document(id='5eac05f2-73e6-4939-a164-79f88aefc1f3', metadata={}, page_content='Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora.[9][10][11][12] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[7][13][14]'),\n",
       " Document(id='cc38adbf-7f9a-445d-aecb-677591919651', metadata={}, page_content=\"Modalities\\n----------\\n\\nA generative AI system is constructed by applying unsupervised machine learning (invoking  for instance neural network architectures such as generative adversarial networks (GANs), variation autoencoders (VAEs), transformers, or self-supervised machine learning trained on a dataset. The capabilities of a generative AI system depend on the modality or type of the data set used. Generative AI can be either unimodal or multimodal; unimodal systems take only one type of input, whereas multimodal systems can take more than one type of input.[59] For example, one version of OpenAI's GPT-4 accepts both text and image inputs.[60]\\n\\n\\nText\\n----\"),\n",
       " Document(id='f766411d-f4da-4171-8af3-5b582c84bfce', metadata={}, page_content=\"Since its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[23] The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.[24][25] The tradition of creative automations has flourished throughout history, exemplified by Maillardet's automaton created in the\"),\n",
       " Document(id='f023157a-8405-4652-84fd-182457abfe8d', metadata={}, page_content='Many AI music generators have been created that can be generated using a text phrase, genre options, and looped libraries of bars and riffs.[75]\\n\\n\\nVideo\\n-----\\n\\nGenerative AI trained on annotated video can generate temporally-coherent, detailed and photorealistic video clips. Examples include Sora by OpenAI,[12] Runway,[76] and Make-A-Video by Meta Platforms.[77]\\n\\n\\nActions\\n-------')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(question):\n",
    "    retrieved_docs = vector_store.search(query=question, k=5, search_type=\"similarity\")\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in retrieved_docs]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Create Full RAG Chain ======\n",
    "full_chain = {\n",
    "    \"question_type\": chain,\n",
    "    \"context\": lambda x: retrieve(x['user_input']),  # Retrieve relevant documents\n",
    "    \"user_input\": lambda x: x['user_input']\n",
    "} | RunnableLambda(rout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing to: Question Chain (Using Llama3.2)\n",
      "Transformers have several advantages over Recurrent Neural Networks (RNNs):\n",
      "\n",
      "1. **Parallelization**: Transformers can process sequences in parallel, leveraging multiple GPU cores or even entire machines to speed up inference time. In contrast, RNNs are inherently sequential and require more computational resources for longer sequences.\n",
      "\n",
      "2. **Long-range dependencies**: Transformers can capture long-range dependencies more effectively than RNNs due to their attention mechanism, which allows the model to weigh the importance of different parts of the input sequence relative to each other.\n",
      "\n",
      "3. **Scalability**: Transformers can handle larger sequence lengths and more complex data sets than RNNs, making them a popular choice for natural language processing tasks like machine translation and text classification.\n",
      "\n",
      "4. **Efficiency in terms of training time**: While transformers are computationally expensive during training due to the need for large memory buffers, they often converge faster than RNNs.\n",
      "\n",
      "5. **No recurrent connections**: Transformers do not rely on recurrent connections, which can be a source of vanishing or exploding gradients in RNNs, leading to difficulty in training deep models.\n",
      "\n",
      "6. **Fewer parameters and better performance**: Some studies have shown that transformers with fewer parameters can outperform RNNs with many more parameters due to their ability to learn more efficient representations.\n",
      "\n",
      "While RNNs were pioneering work in sequential modeling, transformers have become the new standard for tasks requiring long-range dependencies, such as machine translation, text summarization, and question answering.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Explain why transformers are better than RNNs.\"\n",
    "output = full_chain.invoke({'user_input': user_input})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
